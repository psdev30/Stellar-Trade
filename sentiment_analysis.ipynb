{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f7c49c-e647-44cc-bee7-4da1ea8167b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prateekjukalkar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prateekjukalkar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/prateekjukalkar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "td = pd.read_csv('resources/all-data.csv', encoding=\"iso-8859-1\", header=None)\n",
    "td.drop_duplicates()\n",
    "td.columns = [\"sentiment\", \"headline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3ac856-cb21-42f3-bc95-5208ed982f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "1363 28.1 % \n",
      "\n",
      "neutral\n",
      "2879 59.4 % \n",
      "\n",
      "negative\n",
      "604 12.5 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we want 604 negatives, 800 positives, and 1000 neutrals\n",
    "for s in set(td['sentiment']):\n",
    "    print(s)\n",
    "    print( sum(td['sentiment'] == s),  round( sum(td['sentiment'] == s) / len(td) * 100, 1), '%', '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c7bc74-455d-4ec2-a828-58fb2e7864e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_positive_rows = 800\n",
    "desired_neutral_rows = 1000\n",
    "\n",
    "# Filter the DataFrame to include only positive rows\n",
    "positive_rows = td[td['sentiment'] == 'positive']\n",
    "neutral_rows = td[td['sentiment'] == 'neutral']\n",
    "\n",
    "# If there are more than 800 positive rows, randomly select 800\n",
    "if len(positive_rows) > desired_positive_rows:\n",
    "    selected_positive_rows = positive_rows.sample(n=desired_positive_rows, random_state=42)  # You can choose a different random_state for reproducibility\n",
    "    # Update the original DataFrame with the selected positive rows\n",
    "    td = pd.concat([td[td['sentiment'] != 'positive'], selected_positive_rows])\n",
    "else:\n",
    "    selected_positive_rows = positive_rows\n",
    "    \n",
    "# If there are more than 1000 neutral rows, randomly select 1000\n",
    "if len(neutral_rows) > desired_neutral_rows:\n",
    "    selected_neutral_rows = neutral_rows.sample(n=desired_neutral_rows, random_state=42)  # You can choose a different random_state for reproducibility\n",
    "    # Update the original DataFrame with the selected positive rows\n",
    "    td = pd.concat([td[td['sentiment'] != 'neutral'], selected_neutral_rows])\n",
    "else:\n",
    "    selected_neutral_rows = neutral_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3f3ccb-1fef-445e-8a33-d48eccf040db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2       the international electronic industry company ...\n",
       "415     a tinyurl link takes users to a scamming site ...\n",
       "421     compared with the ftse 100 index  which rose 3...\n",
       "423     compared with the ftse 100 index  which rose 9...\n",
       "500     one of the challenges in the oil production in...\n",
       "                              ...                        \n",
       "3175    the casing comprises a first side casing membe...\n",
       "1156    according to seikku  the retail sector in finl...\n",
       "3037    mreal corporation stock exchange release 27 au...\n",
       "1027    at capman haavisto will be responsible for gro...\n",
       "3288    the name of the newspaper publishing and print...\n",
       "Name: headline, Length: 2404, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make all text lowercase\n",
    "td['headline'] = td['headline'].str.lower()\n",
    "\n",
    "# Create a translation table to remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "td['headline'] = td['headline'].str.translate(translator)\n",
    "\n",
    "# Remove special characters\n",
    "def remove_special_characters(sentence):\n",
    "    # Use regex to remove non-alphanumeric characters\n",
    "    cleaned_sentence = re.sub(r'[^A-Za-z0-9 ]+', '', sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "td['headline'] = td['headline'].apply(remove_special_characters)\n",
    "\n",
    "td['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f58752-1fbc-4cb0-9ef6-c9336fa25af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2       [[i, n, t, e, r, n, a, t, i, o, n, a, l], [e, ...\n",
       "415     [[t, i, n, y, u, r, l], [l, i, n, k], [t, a, k...\n",
       "421     [[c, o, m, p, a, r, e, d], [f, t, s, e], [1, 0...\n",
       "423     [[c, o, m, p, a, r, e, d], [f, t, s, e], [1, 0...\n",
       "500     [[o, n, e], [c, h, a, l, l, e, n, g, e], [o, i...\n",
       "                              ...                        \n",
       "3175    [[c, a, s, i, n, g], [c, o, m, p, r, i, s, e, ...\n",
       "1156    [[a, c, c, o, r, d, i, n, g], [s, e, i, k, k, ...\n",
       "3037    [[m, r, e, a, l], [c, o, r, p, o, r, a, t, i, ...\n",
       "1027    [[c, a, p, m, a, n], [h, a, a, v, i, s, t, o],...\n",
       "3288    [[n, a, m, e], [n, e, w, s, p, a, p, e, r], [p...\n",
       "Name: headline, Length: 2404, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "td['headline'] = td['headline'].str.split()\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "td['headline'] = [[word for word in tokens if word.lower() not in stop_words] for tokens in td['headline']]\n",
    "\n",
    "td['headline'] = td['headline'].apply( lambda x: [lemmatizer.lemmatize(word) for word in x] )\n",
    "\n",
    "# Tokenize by letter\n",
    "def letter_tokenize(word):\n",
    "    x = []\n",
    "    for i in word:\n",
    "        x.append(list(i))\n",
    "    return x\n",
    "    \n",
    "td['headline'] = td['headline'].apply(letter_tokenize)\n",
    "td['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60606100-b15d-4853-8d78-a9216f19c52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2       [[57, 62, 68, 53, 66, 62, 49, 68, 57, 63, 62, ...\n",
       "415     [[68, 57, 62, 73, 69, 66, 60], [60, 57, 62, 59...\n",
       "421     [[51, 63, 61, 64, 49, 66, 53, 52], [54, 68, 67...\n",
       "423     [[51, 63, 61, 64, 49, 66, 53, 52], [54, 68, 67...\n",
       "500     [[63, 62, 53], [51, 56, 49, 60, 60, 53, 62, 55...\n",
       "                              ...                        \n",
       "3175    [[51, 49, 67, 57, 62, 55], [51, 63, 61, 64, 66...\n",
       "1156    [[49, 51, 51, 63, 66, 52, 57, 62, 55], [67, 53...\n",
       "3037    [[61, 66, 53, 49, 60], [51, 63, 66, 64, 63, 66...\n",
       "1027    [[51, 49, 64, 61, 49, 62], [56, 49, 49, 70, 57...\n",
       "3288    [[62, 49, 61, 53], [62, 53, 71, 67, 64, 49, 64...\n",
       "Name: headline, Length: 2404, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_numbers(td):\n",
    "    return td['headline'].apply(lambda sublist: [[ord(letter) - ord(\"0\") for letter in word] for word in sublist])\n",
    "\n",
    "td['headline'] = convert_to_numbers(td)\n",
    "td['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6061ef-bb65-4bd3-b471-8ac8bba5d8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2       [57, 62, 68, 53, 66, 62, 49, 68, 57, 63, 62, 4...\n",
      "415     [68, 57, 62, 73, 69, 66, 60, 60, 57, 62, 59, 6...\n",
      "421     [51, 63, 61, 64, 49, 66, 53, 52, 54, 68, 67, 5...\n",
      "423     [51, 63, 61, 64, 49, 66, 53, 52, 54, 68, 67, 5...\n",
      "500     [63, 62, 53, 51, 56, 49, 60, 60, 53, 62, 55, 5...\n",
      "                              ...                        \n",
      "3175    [51, 49, 67, 57, 62, 55, 51, 63, 61, 64, 66, 5...\n",
      "1156    [49, 51, 51, 63, 66, 52, 57, 62, 55, 67, 53, 5...\n",
      "3037    [61, 66, 53, 49, 60, 51, 63, 66, 64, 63, 66, 4...\n",
      "1027    [51, 49, 64, 61, 49, 62, 56, 49, 49, 70, 57, 6...\n",
      "3288    [62, 49, 61, 53, 62, 53, 71, 67, 64, 49, 64, 5...\n",
      "Name: headline, Length: 2404, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# flatten the inner arrays\n",
    "def combine_inner_arrays(row):\n",
    "    return sum(row, [])\n",
    "\n",
    "td['headline'] = td['headline'].apply(combine_inner_arrays)\n",
    "\n",
    "print(td['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d7894a3-b0b2-4145-9e9a-3a836d4196ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the inner array of each headline so they're all the same length\n",
    "\n",
    "# first, lets find the max length of any of the inner arrays as a baseline\n",
    "max_len = td['headline'].apply(len).max()\n",
    "# pad the rest of the inner arrays with 0s\n",
    "td['headline'] = td['headline'].apply(lambda x: x + [0] * (max_len - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d871b4-69d2-42af-ab9c-68cf41375514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove max colwidth to verify if padding was successful\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# # Print the DataFrame\n",
    "# print(td.to_string(index=False))\n",
    "\n",
    "# pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db6f5ac5-955a-451e-ac3d-cc86877e6781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2       0\n",
       "415     0\n",
       "421     0\n",
       "423     0\n",
       "500     0\n",
       "       ..\n",
       "3175    2\n",
       "1156    2\n",
       "3037    2\n",
       "1027    2\n",
       "3288    2\n",
       "Name: sentiment, Length: 2404, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di = {\"positive\":1, \"negative\":0, \"neutral\":2}\n",
    "td[\"sentiment\"].replace(di, inplace=True)\n",
    "\n",
    "td[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c89499c4-b58b-4c7b-bde8-a53702350771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1404, 231), (1404, 1), numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = td[td[\"sentiment\"] != 2]\n",
    "\n",
    "X = np.vstack( td['headline'] )\n",
    "y = np.vstack( td['sentiment'] )\n",
    "# X = np.expand_dims(X, 2)\n",
    "\n",
    "X.shape, y.shape, type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c07de64-054c-4435-a924-76e3a8f2ecbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d3225a-9bb6-453d-bb20-c3c42124c93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1333, 231), (71, 231), (1333, 2), (71, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=17)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea15fa13-a0cc-4077-8aec-193455b35a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Define LSTM model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Embedding(231, 231),\n",
    "#     tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "#     tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#     tf.keras.layers.Reshape((1, 128)),\n",
    "#     tf.keras.layers.LSTM(128, activation='relu'),\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(3, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72cbb7cc-32dd-4e50-a080-3ee7972aefb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 231)         53361     \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 512)         591872    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 256)         1310976   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, None, 128)         655488    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 128)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2751667 (10.50 MB)\n",
      "Trainable params: 2751667 (10.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "tf.keras.layers.Embedding(231, 231),\n",
    "tf.keras.layers.Conv1D(512, 5, activation='relu'),\n",
    "tf.keras.layers.Conv1D(256, 10, activation='relu'),\n",
    "tf.keras.layers.Conv1D(128, 20, activation='relu'),\n",
    "tf.keras.layers.GlobalAveragePooling1D(),\n",
    "tf.keras.layers.Reshape((1, 128)),\n",
    "tf.keras.layers.LSTM(128),\n",
    "tf.keras.layers.Dense(64, activation='relu'),\n",
    "tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cf9b189-1431-4116-a3e4-8c50194ca122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "42/42 [==============================] - 15s 347ms/step - loss: 0.6800 - accuracy: 0.5694\n",
      "Epoch 2/20\n",
      "42/42 [==============================] - 14s 328ms/step - loss: 0.6581 - accuracy: 0.5754\n",
      "Epoch 3/20\n",
      "42/42 [==============================] - 14s 328ms/step - loss: 0.6406 - accuracy: 0.6219\n",
      "Epoch 4/20\n",
      "42/42 [==============================] - 14s 331ms/step - loss: 0.6216 - accuracy: 0.6512\n",
      "Epoch 5/20\n",
      "42/42 [==============================] - 14s 329ms/step - loss: 0.5879 - accuracy: 0.6864\n",
      "Epoch 6/20\n",
      "42/42 [==============================] - 14s 329ms/step - loss: 0.6002 - accuracy: 0.6647\n",
      "Epoch 7/20\n",
      "42/42 [==============================] - 14s 336ms/step - loss: 0.5309 - accuracy: 0.7457\n",
      "Epoch 8/20\n",
      "42/42 [==============================] - 14s 333ms/step - loss: 0.4825 - accuracy: 0.7659\n",
      "Epoch 9/20\n",
      "42/42 [==============================] - 14s 331ms/step - loss: 0.4295 - accuracy: 0.8050\n",
      "Epoch 10/20\n",
      "42/42 [==============================] - 14s 330ms/step - loss: 0.3702 - accuracy: 0.8507\n",
      "Epoch 11/20\n",
      "42/42 [==============================] - 14s 328ms/step - loss: 0.3120 - accuracy: 0.8725\n",
      "Epoch 12/20\n",
      "42/42 [==============================] - 14s 330ms/step - loss: 0.2682 - accuracy: 0.8957\n",
      "Epoch 13/20\n",
      "42/42 [==============================] - 14s 328ms/step - loss: 0.2201 - accuracy: 0.9152\n",
      "Epoch 14/20\n",
      "42/42 [==============================] - 14s 333ms/step - loss: 0.1931 - accuracy: 0.9302\n",
      "Epoch 15/20\n",
      "42/42 [==============================] - 14s 330ms/step - loss: 0.1724 - accuracy: 0.9400\n",
      "Epoch 16/20\n",
      "42/42 [==============================] - 14s 329ms/step - loss: 0.1787 - accuracy: 0.9392\n",
      "Epoch 17/20\n",
      "42/42 [==============================] - 14s 331ms/step - loss: 0.1266 - accuracy: 0.9557\n",
      "Epoch 18/20\n",
      "42/42 [==============================] - 14s 331ms/step - loss: 0.1481 - accuracy: 0.9407\n",
      "Epoch 19/20\n",
      "42/42 [==============================] - 14s 330ms/step - loss: 0.0685 - accuracy: 0.9805\n",
      "Epoch 20/20\n",
      "42/42 [==============================] - 14s 332ms/step - loss: 0.0747 - accuracy: 0.9790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x296ebf810>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9730744-8f66-4776-92d3-2392c26ae4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"finance_sentiment.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccc1f186-f894-43fc-afc0-78180991da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 57ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({1: 43, 0: 28})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter( np.argmax( model.predict(X_test), axis=1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9aa88f9-a765-4e2f-a6df-4bf9a373cdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 40, 0: 31})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter( list( np.argmax( y_test , axis=1) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71112c94-b27b-4399-a073-57738273fb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[9.999099e-01, 9.014498e-05]], dtype=float32),\n",
       " array([1., 0.], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[0:1]), y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba92b65c-8230-4501-af12-72e34c84a5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 55ms/step - loss: 0.6369 - accuracy: 0.7887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.636885404586792, 0.7887324094772339]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "737a84ef-91e4-49af-ba30-ca247b276fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9999099, array([0]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(X_test[0:1])\n",
    "\n",
    "max(prediction[0]), np.argmax(prediction, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
